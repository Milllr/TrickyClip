# TrickyClip Production System - Complete Guide

## ğŸ‰ What Was Built

Your TrickyClip system is now a production-ready, ML-powered video processing platform with:

### Core Features
- âœ… **ML-based trick detection** - motion analysis finds tricks automatically
- âœ… **Professional timeline scrubber** - drag endpoints, keyboard shortcuts
- âœ… **Google Drive integration** - automatic folder organization
- âœ… **Persistent job tracking** - survives restarts, full history
- âœ… **Responsive UI** - adapts to mobile/desktop and portrait/landscape videos
- âœ… **Autocomplete** - searchable person/trick dropdowns
- âœ… **Enhanced filenames** - includes resolution, aspect ratio, fps
- âœ… **Clips library** - search, filter, view all saved clips
- âœ… **Chunked uploads** - handle 100GB+ files without timeout
- âœ… **Parallel workers** - 2 workers x 2 jobs = 4 concurrent processing
- âœ… **Automated storage** - cleans up uploaded clips to save space
- âœ… **Health monitoring** - `/health`, `/health/ready`, `/health/metrics`
- âœ… **Database backups** - automated backup/restore scripts
- âœ… **One-command deploy** - `./deploy/deploy.sh` updates everything

## ğŸš€ Deployment Instructions

### Step 1: Upload All Code to VM

From your Mac:

```bash
cd /Users/kahuna/code/TrickyClip
chmod +x deploy/*.sh
./deploy/deploy.sh
```

This automatically:
1. Uploads backend, frontend, deploy files
2. Runs database migrations
3. Rebuilds containers
4. Restarts services

### Step 2: Run Database Migration on VM

SSH to VM first time only to set up schema:

```bash
gcloud compute ssh kahuna@trickyclip-server --zone=us-central1-c
```

On VM:

```bash
cd /opt/trickyclip/deploy

# run migrations to add new tables/columns
docker compose up -d db
sleep 5
docker compose exec db psql -U trickyclip trickyclip << 'EOF'
-- add new columns to original_files
ALTER TABLE original_files ADD COLUMN IF NOT EXISTS width INTEGER DEFAULT 0;
ALTER TABLE original_files ADD COLUMN IF NOT EXISTS height INTEGER DEFAULT 0;
ALTER TABLE original_files ADD COLUMN IF NOT EXISTS aspect_ratio VARCHAR DEFAULT 'unknown';
ALTER TABLE original_files ADD COLUMN IF NOT EXISTS resolution_label VARCHAR DEFAULT 'unknown';
ALTER TABLE original_files ADD COLUMN IF NOT EXISTS processing_status VARCHAR DEFAULT 'pending';
ALTER TABLE original_files ADD COLUMN IF NOT EXISTS analysis_progress_percent INTEGER DEFAULT 0;

-- add new columns to candidate_segments
ALTER TABLE candidate_segments ADD COLUMN IF NOT EXISTS confidence_score FLOAT DEFAULT 0.5;
ALTER TABLE candidate_segments ADD COLUMN IF NOT EXISTS detection_method VARCHAR DEFAULT 'basic';

-- add new columns to final_clips
ALTER TABLE final_clips ADD COLUMN IF NOT EXISTS resolution_label VARCHAR DEFAULT 'unknown';
ALTER TABLE final_clips ADD COLUMN IF NOT EXISTS aspect_ratio VARCHAR DEFAULT 'unknown';
ALTER TABLE final_clips ADD COLUMN IF NOT EXISTS drive_url VARCHAR;
ALTER TABLE final_clips ADD COLUMN IF NOT EXISTS is_uploaded_to_drive BOOLEAN DEFAULT false;
ALTER TABLE final_clips ADD COLUMN IF NOT EXISTS clip_hash VARCHAR;

-- create jobs table
CREATE TABLE IF NOT EXISTS jobs (
    id UUID PRIMARY KEY,
    rq_job_id VARCHAR UNIQUE NOT NULL,
    job_type VARCHAR NOT NULL,
    status VARCHAR NOT NULL,
    file_id UUID,
    clip_id UUID,
    progress_percent INTEGER DEFAULT 0,
    error_message TEXT,
    started_at TIMESTAMP,
    finished_at TIMESTAMP,
    created_at TIMESTAMP NOT NULL,
    updated_at TIMESTAMP NOT NULL
);

CREATE INDEX IF NOT EXISTS ix_jobs_rq_job_id ON jobs(rq_job_id);
CREATE INDEX IF NOT EXISTS ix_jobs_status ON jobs(status);
CREATE INDEX IF NOT EXISTS ix_jobs_file_id ON jobs(file_id);
CREATE INDEX IF NOT EXISTS ix_jobs_clip_id ON jobs(clip_id);
EOF

# rebuild and restart all services
docker compose down
docker compose up -d --build

# verify
docker compose ps
```

### Step 3: Set Up Automated Cleanup

On VM, add cron job for storage management:

```bash
# edit crontab
crontab -e

# add this line (runs every 6 hours):
0 */6 * * * /opt/trickyclip/deploy/cleanup-cron.sh
```

### Step 4: Set Up Automated Backups

On VM:

```bash
# add daily backup at 3am
crontab -e

# add this line:
0 3 * * * /opt/trickyclip/deploy/backup-database.sh
```

## ğŸ“– How to Use TrickyClip

### Upload Videos (Bulk Processing)

1. Go to **https://trickyclip.com/upload**
2. Select multiple videos (even 100GB+)
3. System automatically:
   - Uploads with chunked transfer for large files
   - Queues analysis jobs
   - Detects tricks using ML motion detection
   - Creates candidate segments with confidence scores

### Sort Clips (The Magic Part)

1. Go to **https://trickyclip.com/sort**
2. You'll see:
   - Video player with the detected segment
   - Timeline scrubber with draggable handles
   - 2-second buffer on each side for fine-tuning
   - Autocomplete person/trick fields
   - Session name field
   
3. Controls:
   - **Drag timeline handles** to adjust clip boundaries
   - **Spacebar** to play/pause
   - **Cmd/Ctrl+S** to save & next
   - **Cmd/Ctrl+D** to trash

4. When you save:
   - Clip renders via FFmpeg
   - Uploads to Google Drive in structured folders
   - Deletes local file to save VM space
   - Shows in /clips page

### View Saved Clips

1. Go to **https://trickyclip.com/clips**
2. Features:
   - Search by person name, trick, date, resolution
   - Filter by year, category, resolution
   - Grid/list/tree views
   - Click to view in Google Drive
   - Statistics dashboard

### Monitor Background Jobs

1. Go to **https://trickyclip.com/jobs**
2. See real-time:
   - Running jobs with progress
   - Queued jobs waiting
   - Completed job history
   - Failed jobs with errors
3. Auto-refreshes every 2 seconds

## ğŸ—‚ï¸ Google Drive Folder Structure

Your clips are organized as:

```
TrickyClip Archive/
â”œâ”€â”€ 2025/
â”‚   â”œâ”€â”€ 2025-12-02_Session1/
â”‚   â”‚   â”œâ”€â”€ miller-downeyTricks/
â”‚   â”‚   â”‚   â”œâ”€â”€ kickflip/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ 2025-12-02__Session1__miller-downey__kickflip__CAM1__1080p__9x16__60FPS__v001.mp4
â”‚   â”‚   â”‚   â””â”€â”€ backflip/
â”‚   â”‚   â”‚       â””â”€â”€ 2025-12-02__Session1__miller-downey__backflip__CAM1__1080p__16x9__120FPS__v001.mp4
â”‚   â”‚   â””â”€â”€ BROLLTricks/
â”‚   â”‚       â””â”€â”€ BROLL/
â”‚   â”‚           â””â”€â”€ 2025-12-02__Session1__BROLL__BROLL__CAM1__720p__16x9__30FPS__v001.mp4
â”‚   â””â”€â”€ 2025-12-15_Session2/
â”‚       â””â”€â”€ ...
â””â”€â”€ 2024/
    â””â”€â”€ ...
```

## ğŸ”§ API Endpoints

### Health & Monitoring
- `GET /health` - liveness check
- `GET /health/ready` - dependency health
- `GET /health/metrics` - prometheus metrics

### Upload
- `POST /api/upload/` - standard upload (small files)
- `POST /api/upload-chunked/init` - start chunked upload
- `POST /api/upload-chunked/chunk/{id}` - upload chunk
- `POST /api/upload-chunked/complete/{id}` - finalize
- `GET /api/upload/media/{file_id}` - stream video

### Sorting
- `GET /api/sort/next` - get next segment (high confidence first)
- `POST /api/sort/save` - save clip and render
- `POST /api/sort/trash` - mark segment as trashed

### Clips
- `GET /api/clips/` - list clips with filters
- `GET /api/clips/stats` - statistics
- `GET /api/clips/tree` - folder tree structure

### Jobs
- `GET /api/jobs/` - list all jobs with status
- Real-time updates via database tracking

### Admin
- `GET /api/admin/storage` - disk usage stats
- `POST /api/admin/storage/cleanup` - trigger cleanup
- `POST /api/admin/reprocess/{file_id}` - reprocess file

### People & Tricks
- `GET /api/people/` - list all people
- `POST /api/people/` - add person
- `GET /api/tricks/` - list all tricks
- `POST /api/tricks/` - add trick

### WebSocket
- `WS /ws/progress` - real-time job progress updates

## ğŸ¯ Processing Your 100GB Backlog

### Recommended Workflow

1. **Upload in batches** (10-20 GB at a time):
   ```
   - Upload Monday
   - Let it process overnight
   - Sort Tuesday
   - Upload next batch Tuesday evening
   ```

2. **ML detection prioritizes best clips:**
   - High-confidence segments appear first
   - Skip low-quality segments quickly
   - Focus your time on good tricks

3. **Monitor progress:**
   - Check `/jobs` to see processing status
   - Check `/health/metrics` for overall statistics
   - Watch worker logs: `docker compose logs worker -f`

4. **Storage auto-manages:**
   - Uploaded clips deleted after Drive upload
   - Old originals deleted after 30 days
   - LRU eviction if disk >80% full
   - Runs automatically every 6 hours

## ğŸ› ï¸ Maintenance Commands

### Deploy Updates
```bash
./deploy/deploy.sh
```

### Rollback
```bash
./deploy/rollback.sh
```

### Backup Database
```bash
ssh to VM
/opt/trickyclip/deploy/backup-database.sh
```

### Restore Database
```bash
ssh to VM
/opt/trickyclip/deploy/restore-database.sh /opt/trickyclip/backups/trickyclip_backup_20251202_030000.sql.gz
```

### Manual Cleanup
```bash
curl -X POST http://localhost:8001/api/admin/storage/cleanup -H "Content-Type: application/json" -d '{"aggressive": false}'
```

### Check Health
```bash
curl http://localhost:8001/health/metrics
```

### Scale Workers

Edit `deploy/docker-compose.yml`:

```yaml
worker:
  deploy:
    replicas: 4  # increase from 2 to 4 for 8 concurrent jobs
```

Then restart:
```bash
docker compose up -d --scale worker=4
```

## ğŸ’° Cost Optimization Tips

### During Free Trial
- Use e2-standard-4 (4 vCPU, 16GB) - great for video processing
- Process everything during 90-day trial
- Let storage auto-cleanup manage disk usage

### After Free Trial
- Downgrade to e2-standard-2 (2 vCPU, 8GB) - $50/month
- Or e2-micro (0.25-2 vCPU, 1GB) - $7/month for light usage
- Reduce worker replicas to 1
- Enable auto-shutdown during off-hours

### Storage Cost Reduction
- All clips stored on Google Drive (15GB free, then $2/month per 100GB)
- VM only holds temporary files
- Auto-cleanup every 6 hours
- Can manually trigger: `/api/admin/storage/cleanup`

## ğŸ¬ Filename Convention

Every saved clip follows this pattern:

```
2025-12-02__Session1__miller-downey__kickflip__CAM1__1080p__9x16__60FPS__v001.mp4
â”‚           â”‚         â”‚              â”‚          â”‚     â”‚      â”‚     â”‚      â”‚
â”‚           â”‚         â”‚              â”‚          â”‚     â”‚      â”‚     â”‚      â””â”€ version number
â”‚           â”‚         â”‚              â”‚          â”‚     â”‚      â”‚     â””â”€ fps
â”‚           â”‚         â”‚              â”‚          â”‚     â”‚      â””â”€ aspect ratio
â”‚           â”‚         â”‚              â”‚          â”‚     â””â”€ resolution
â”‚           â”‚         â”‚              â”‚          â””â”€ camera
â”‚           â”‚         â”‚              â””â”€ trick name
â”‚           â”‚         â””â”€ person slug
â”‚           â””â”€ session name
â””â”€ date
```

Search using "Everything" app:
- `miller-downey` â†’ all tricks by that person
- `kickflip` â†’ all kickflips by anyone
- `2025-12-02` â†’ all tricks from that date
- `1080p` â†’ all 1080p clips
- `9x16` â†’ all portrait clips

## ğŸ”¥ Current Issues Fixed

1. âœ… **Clips now upload to Drive** - no longer stored only locally
2. âœ… **Clips page works** - view, search, filter all saved clips
3. âœ… **Jobs persist** - database tracking survives restarts
4. âœ… **ML detection** - finds tricks intelligently, not just time-based
5. âœ… **Better UI** - professional scrubber, responsive layout
6. âœ… **Storage managed** - auto-cleanup prevents disk full

## ğŸš¦ Next Steps

### 1. Deploy Everything

```bash
cd /Users/kahuna/code/TrickyClip
./deploy/deploy.sh
```

### 2. Run Database Migration

Follow instructions in Step 2 above to add new columns/tables.

### 3. Test with Sample Videos

Upload 2-3 videos to test:
- ML detection quality
- Sort page UI/UX
- Drive upload
- Clips page display

### 4. Adjust ML Parameters

If detection is too sensitive/not sensitive enough, edit on VM:

```bash
# edit detection_ml.py
nano /opt/trickyclip/backend/app/services/detection_ml.py

# adjust these values:
min_motion_threshold = 0.25  # lower = more segments
min_segment_duration_ms = 800  # minimum trick length
max_segment_duration_ms = 8000  # maximum trick length

# restart
docker compose restart backend worker
```

### 5. Start Bulk Upload

Once tuned, upload your 100GB in batches:
- 10-20 GB per day
- Let process overnight
- Sort during the day
- Repeat until done

## ğŸ“Š Monitoring Dashboard

Visit these URLs:

- **https://trickyclip.com** - home
- **https://trickyclip.com/upload** - upload videos
- **https://trickyclip.com/sort** - sort clips
- **https://trickyclip.com/jobs** - monitor processing
- **https://trickyclip.com/clips** - view saved clips
- **https://trickyclip.com/health/metrics** - system metrics

## âš¡ Performance Expectations

With e2-standard-4 VM (4 vCPU, 16GB RAM):
- **Upload:** ~500 MB/min (depends on internet)
- **ML Detection:** ~30-60 seconds per 10min video
- **Rendering:** ~5-10 seconds per clip
- **Drive Upload:** ~20-30 seconds per clip
- **Concurrent:** 4 videos processing simultaneously

Total throughput: ~200-300 GB per day if running 24/7

## ğŸ› Troubleshooting

### Clips Not Showing Up
```bash
# check if render/upload is working
docker compose logs worker --tail=100

# check clips in database
curl http://localhost:8001/api/clips/stats
```

### Jobs Not Processing
```bash
# check worker is running
docker compose ps worker

# check Redis connection
docker compose logs redis

# manually trigger reprocess
curl -X POST http://localhost:8001/api/admin/reprocess/{file_id}
```

### Drive Upload Failing
```bash
# verify credentials
docker compose exec backend ls -la /opt/trickyclip/secrets/

# test connection
docker compose exec backend python -c "from app.services.drive import drive_service; print('OK' if drive_service.service else 'FAIL')"
```

### Out of Disk Space
```bash
# check usage
curl http://localhost:8001/api/admin/storage

# trigger cleanup
curl -X POST http://localhost:8001/api/admin/storage/cleanup -H "Content-Type: application/json" -d '{"aggressive": true}'
```

## ğŸ¨ UI Customization

All frontend styling uses Tailwind CSS. To customize:

1. Edit `/Users/kahuna/code/TrickyClip/frontend/src/pages/*.tsx`
2. Run `./deploy/deploy.sh`
3. Changes appear immediately

## ğŸ“ˆ Scaling Strategy

### Phase 1: Free Trial (90 days)
- Use e2-standard-4
- Process entire 100GB backlog
- Run 24/7 with 2 workers

### Phase 2: Post-Trial
- Downgrade to e2-standard-2 ($50/month)
- Reduce to 1 worker
- Process new videos weekly

### Phase 3: Low-Cost Maintenance
- Downgrade to e2-small ($25/month)
- Auto-shutdown nights (save 50%)
- Process on-demand only

## ğŸ“ Tips for Efficient Sorting

1. **Use keyboard shortcuts:**
   - Space to play/pause
   - Cmd+S to save & next
   - Cmd+D to trash

2. **Drag timeline handles** instead of typing times

3. **Autocomplete is fast:**
   - Type first few letters
   - Click from dropdown
   - No need to type full names

4. **High-confidence clips first:**
   - ML puts best detections at top
   - Sort those first
   - Skip low-quality ones

5. **Batch similar tricks:**
   - Sort all kickflips at once
   - Muscle memory kicks in
   - Go faster over time

## ğŸŒŸ What Makes This Special

Your system now:
- **Runs 24/7** without your laptop
- **Intelligently detects tricks** using ML
- **Organizes perfectly** on Google Drive
- **Handles massive files** with chunked upload
- **Never loses progress** - database tracking
- **Self-manages storage** - automated cleanup
- **Scales effortlessly** - from 1 to 10 workers
- **Costs almost nothing** - free trial then ~$25-50/month

You can upload 100GB today, let it process, and sort clips whenever you have 5 minutes free. The system remembers everything.

## ğŸš¨ Important Notes

1. **First deployment** requires running the SQL migration (Step 2)
2. **Test with small videos** first to tune ML parameters
3. **Monitor storage** via `/api/admin/storage` 
4. **Backup database** before major changes
5. **Worker logs** show all processing activity

Ready to transform your footage management! ğŸ¿

